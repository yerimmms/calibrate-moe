{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzX/oACPPw2vnLXSwbjoDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yerimmms/calibrate-moe/blob/calibrate-moe/humalevalpack_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BAGIs1avxGX",
        "outputId": "f0a99e16-a57b-471b-b35d-ad37d50cd8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bigcode-evaluation-harness'...\n",
            "remote: Enumerating objects: 4282, done.\u001b[K\n",
            "remote: Counting objects: 100% (1759/1759), done.\u001b[K\n",
            "remote: Compressing objects: 100% (561/561), done.\u001b[K\n",
            "remote: Total 4282 (delta 1380), reused 1424 (delta 1189), pack-reused 2523\u001b[K\n",
            "Receiving objects: 100% (4282/4282), 932.66 KiB | 4.36 MiB/s, done.\n",
            "Resolving deltas: 100% (2919/2919), done.\n",
            "/content/bigcode-evaluation-harness\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bigcode-project/bigcode-evaluation-harness.git\n",
        "%cd bigcode-evaluation-harness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2xnaisowBvp",
        "outputId": "08e4781b-904a-45e9-a722-671caaf524e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "collapsed": true,
        "id": "cXB8fiEMwD-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for TF-TRT Warning: Could not find TensorRT\n",
        "# !pip install tensorflow-gpu==2.8.0"
      ],
      "metadata": {
        "id": "SdXXmIX-3rBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi_KUer7wRdf",
        "outputId": "623ca1f8-fcdd-473a-f459-b69595eeb506"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep40tnxkgeM6",
        "outputId": "2345bc8e-6f38-4ea7-ae6f-bf3e215707b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copy-and-paste the text below in your GitHub issue.\n",
            "\n",
            "- huggingface_hub version: 0.23.2\n",
            "- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "- Python version: 3.10.12\n",
            "- Running in iPython ?: No\n",
            "- Running in notebook ?: No\n",
            "- Running in Google Colab ?: No\n",
            "- Token path ?: /root/.cache/huggingface/token\n",
            "- Has saved token ?: True\n",
            "- Who am I ?: yrju\n",
            "- Configured git credential helpers: \n",
            "- FastAI: 2.7.15\n",
            "- Tensorflow: 2.15.0\n",
            "- Torch: 2.3.0+cu121\n",
            "- Jinja2: 3.1.4\n",
            "- Graphviz: 0.20.3\n",
            "- keras: 2.8.0\n",
            "- Pydot: 1.4.2\n",
            "- Pillow: 9.4.0\n",
            "- hf_transfer: N/A\n",
            "- gradio: N/A\n",
            "- tensorboard: N/A\n",
            "- numpy: 1.25.2\n",
            "- pydantic: 2.7.3\n",
            "- aiohttp: 3.9.5\n",
            "- ENDPOINT: https://huggingface.co\n",
            "- HF_HUB_CACHE: /root/.cache/huggingface/hub\n",
            "- HF_ASSETS_CACHE: /root/.cache/huggingface/assets\n",
            "- HF_TOKEN_PATH: /root/.cache/huggingface/token\n",
            "- HF_HUB_OFFLINE: False\n",
            "- HF_HUB_DISABLE_TELEMETRY: False\n",
            "- HF_HUB_DISABLE_PROGRESS_BARS: None\n",
            "- HF_HUB_DISABLE_SYMLINKS_WARNING: False\n",
            "- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\n",
            "- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\n",
            "- HF_HUB_ENABLE_HF_TRANSFER: False\n",
            "- HF_HUB_ETAG_TIMEOUT: 10\n",
            "- HF_HUB_DOWNLOAD_TIMEOUT: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 환경변수 설정 ###\n",
        "MODEL_NAME=\"yrju/CodeLlaMa-7B-dare-ties\"\n",
        "MAX_LENGTH=512\n",
        "PROMPT=\"instruct\"\n",
        "TASK_NAME=\"humanevalfixtests-python\"\n",
        "PRECISION=\"fp16\"\n",
        "\n",
        "# MODEL_NAME=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# PRECISION=\"bf16\""
      ],
      "metadata": {
        "id": "29vch6xTpO2Z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HumanEvalFix: In this task models are provided with a solution with a subtle bug and several unit tests. The task is to fix the function.\n",
        "# There is a variant of this task where the function docstring instead of the unit tests are provided,\n",
        "# which can be selected via humanevalfixdocs.\n",
        "!accelerate launch main.py \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --max_length_generation {MAX_LENGTH} \\\n",
        "  --prompt {PROMPT} \\\n",
        "  --tasks humanevalfixtests-python \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 20 \\\n",
        "  --batch_size 10 \\\n",
        "  --precision {PRECISION} \\\n",
        "  --allow_code_execution \\\n",
        "  --save_generations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqjce_ggwmzo",
        "outputId": "f0adf50d-55ed-4bcd-fc05-625c2d360325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-06-06 14:11:32.996423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2024-06-06 14:11:32.996459: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Selected Tasks: ['humanevalfixtests-python']\n",
            "Loading model in bf16\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for bigcode/humanevalpack contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigcode/humanevalpack\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "number of problems for this task is 164\n",
            "  4% 14/328 [02:20<56:18, 10.76s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HumanEvalExplain: In this task models need to explain a HumanEval solution (without docstring)\n",
        "# and subsequently regenerate the solution given only the model's own explanation.\n",
        "# Thus, it requires two runs.\n",
        "# The first one generates the descriptions, the second loads the descriptions, generates the solution & is scored.\n",
        "\n",
        "!accelerate launch main.py \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --max_length_generation {MAX_LENGTH} \\\n",
        "  --prompt {PROMPT} \\\n",
        "  --tasks humanevalexplaindescribe-python \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 20 \\\n",
        "  --batch_size 10 \\\n",
        "  --generation_only \\\n",
        "  --save_generations\n",
        "\n",
        "!accelerate launch main.py \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --max_length_generation {MAX_LENGTH} \\\n",
        "  --prompt {PROMPT} \\\n",
        "  --load_data_path generations_humanevalexplaindescribe-python.json \\\n",
        "  --tasks humanevalexplainsynthesize-python \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 1 \\\n",
        "  --batch_size 1 \\\n",
        "  --allow_code_execution"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qhKr8pWeoRRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HumanEvalSynthesize: This is like HumanEval but with human translations for JavaScript, Java, Go, C++ and Rust.\n",
        "# It is based on HumanEval-X, however, with additional fixes and improvements documented here.\n",
        "!accelerate launch main.py \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --max_length_generation {MAX_LENGTH} \\\n",
        "  --prompt {PROMPT} \\\n",
        "  --tasks humanevalsynthesize-python \\\n",
        "  --temperature 0.2 \\\n",
        "  --n_samples 20 \\\n",
        "  --batch_size 10 \\\n",
        "  --allow_code_execution \\\n",
        "  --save_generations"
      ],
      "metadata": {
        "id": "jCLLqXKtoxSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"yrju/CodeLlaMa-7B-dare-ties\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"yrju/CodeLlaMa-7B-dare-ties\")"
      ],
      "metadata": {
        "id": "Lly5UHt641x_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}