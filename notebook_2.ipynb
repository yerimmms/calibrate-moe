{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "84cRJT6_ecbw"
      },
      "outputs": [],
      "source": [
        "OUTPUT_PATH = \"./ultra_llm_merged\"  # folder to store the result in\n",
        "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
        "CONFIG_YML = \"./examples/ultra_llm_merged.yml\"  # merge configuration file\n",
        "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
        "LAZY_UNPICKLE = True  # experimental low-memory model loader\n",
        "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap\n",
        "ALLOW_CRIMES = True # Allow mixing architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6nw26xQLkBax"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/iddx/workspace/mergekit/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Fetching 10 files: 100%|██████████| 10/10 [04:56<00:00, 29.62s/it]\n",
            "Warmup loader cache: 100%|██████████| 3/3 [04:56<00:00, 98.91s/it]\n",
            "Executing graph:   0%|          | 3/1748 [00:00<05:15,  5.54it/s]WARNING:root:Using submatrix of codellama/CodeLlama-7b-Instruct-hf:lm_head.weight\n",
            "Executing graph:   1%|          | 9/1748 [00:02<06:29,  4.47it/s]WARNING:root:Using submatrix of codellama/CodeLlama-7b-Instruct-hf:model.embed_tokens.weight\n",
            "Executing graph:   1%|▏         | 22/1748 [00:02<02:24, 11.96it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.0.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:   2%|▏         | 28/1748 [00:03<02:08, 13.39it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.0.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:   2%|▏         | 33/1748 [00:03<02:07, 13.48it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.0.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:   2%|▏         | 36/1748 [00:03<01:54, 15.00it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.0.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:   3%|▎         | 58/1748 [00:03<00:41, 41.00it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.0.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:   4%|▍         | 75/1748 [00:04<00:35, 47.02it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.1.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:   5%|▍         | 81/1748 [00:04<00:38, 43.84it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.1.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:   5%|▍         | 86/1748 [00:04<00:54, 30.64it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.1.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:   5%|▌         | 90/1748 [00:05<01:11, 23.31it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.1.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:   6%|▋         | 112/1748 [00:05<00:38, 43.03it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.1.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:   7%|▋         | 128/1748 [00:05<00:25, 63.61it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.10.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:   8%|▊         | 137/1748 [00:05<00:46, 34.55it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.10.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.10.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:   8%|▊         | 144/1748 [00:06<01:01, 25.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.10.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  10%|▉         | 168/1748 [00:06<00:38, 41.03it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.10.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  11%|█         | 184/1748 [00:07<00:33, 46.84it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.11.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  11%|█         | 191/1748 [00:07<00:43, 36.02it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.11.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  11%|█▏        | 197/1748 [00:07<00:54, 28.51it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.11.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  12%|█▏        | 202/1748 [00:07<00:51, 29.89it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.11.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  13%|█▎        | 219/1748 [00:08<00:36, 41.64it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.11.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  13%|█▎        | 232/1748 [00:08<00:26, 57.74it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.12.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  14%|█▎        | 240/1748 [00:08<00:37, 39.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.12.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  14%|█▍        | 247/1748 [00:09<00:48, 30.70it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.12.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  14%|█▍        | 252/1748 [00:09<01:02, 24.07it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.12.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  16%|█▌        | 274/1748 [00:09<00:35, 41.68it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.12.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  17%|█▋        | 290/1748 [00:09<00:23, 60.77it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.13.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  17%|█▋        | 299/1748 [00:10<00:43, 33.05it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.13.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.13.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  18%|█▊        | 306/1748 [00:10<00:57, 25.06it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.13.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  19%|█▉        | 329/1748 [00:11<00:34, 40.71it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.13.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  20%|█▉        | 344/1748 [00:11<00:25, 55.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.14.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  20%|██        | 353/1748 [00:11<00:42, 33.03it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.14.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.14.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  21%|██        | 360/1748 [00:12<00:55, 25.23it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.14.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  22%|██▏       | 384/1748 [00:12<00:34, 39.83it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.14.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  23%|██▎       | 400/1748 [00:12<00:29, 45.36it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.15.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  23%|██▎       | 407/1748 [00:13<00:38, 35.10it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.15.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  24%|██▎       | 413/1748 [00:13<00:46, 28.51it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.15.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  24%|██▍       | 418/1748 [00:13<00:44, 29.78it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.15.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  25%|██▍       | 436/1748 [00:14<00:30, 43.47it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.15.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  26%|██▌       | 452/1748 [00:14<00:20, 64.69it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.16.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  26%|██▋       | 461/1748 [00:14<00:37, 34.24it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.16.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.16.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  27%|██▋       | 468/1748 [00:15<00:50, 25.25it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.16.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  28%|██▊       | 492/1748 [00:15<00:31, 40.15it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.16.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  29%|██▉       | 507/1748 [00:15<00:27, 45.04it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.17.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  29%|██▉       | 514/1748 [00:16<00:35, 34.72it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.17.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  30%|██▉       | 520/1748 [00:16<00:43, 28.07it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.17.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  30%|██▉       | 524/1748 [00:16<00:43, 28.26it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.17.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  31%|███       | 544/1748 [00:17<00:27, 43.97it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.17.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  32%|███▏      | 560/1748 [00:17<00:18, 64.55it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.18.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  33%|███▎      | 569/1748 [00:22<03:19,  5.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.18.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.18.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  33%|███▎      | 576/1748 [00:22<02:51,  6.82it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.18.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  35%|███▍      | 604/1748 [00:23<01:12, 15.70it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.18.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  35%|███▌      | 614/1748 [00:23<01:00, 18.87it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.19.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  36%|███▌      | 622/1748 [00:23<00:58, 19.14it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.19.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  36%|███▌      | 628/1748 [00:24<01:01, 18.26it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.19.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  36%|███▌      | 633/1748 [00:24<00:55, 20.22it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.19.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  37%|███▋      | 654/1748 [00:24<00:30, 35.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.19.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  38%|███▊      | 669/1748 [00:24<00:25, 42.56it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.2.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  39%|███▊      | 676/1748 [00:25<00:33, 32.44it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.2.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  39%|███▉      | 681/1748 [00:25<00:41, 25.79it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.2.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  39%|███▉      | 685/1748 [00:25<00:40, 25.94it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.2.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  40%|████      | 707/1748 [00:26<00:22, 46.11it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.2.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  41%|████      | 721/1748 [00:26<00:16, 63.54it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.20.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  42%|████▏     | 730/1748 [00:26<00:30, 33.65it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.20.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  42%|████▏     | 737/1748 [00:27<00:35, 28.22it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.20.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  43%|████▎     | 743/1748 [00:27<00:32, 30.62it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.20.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  44%|████▎     | 761/1748 [00:27<00:22, 44.06it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.20.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  44%|████▍     | 776/1748 [00:27<00:15, 62.60it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.21.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  45%|████▍     | 785/1748 [00:28<00:28, 33.93it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.21.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.21.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  45%|████▌     | 792/1748 [00:28<00:37, 25.68it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.21.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  47%|████▋     | 815/1748 [00:28<00:21, 42.41it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.21.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  48%|████▊     | 832/1748 [00:29<00:24, 36.89it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.22.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  48%|████▊     | 838/1748 [00:29<00:33, 27.57it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.22.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  48%|████▊     | 843/1748 [00:29<00:32, 27.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.22.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  48%|████▊     | 847/1748 [00:30<00:41, 21.85it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.22.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  50%|████▉     | 867/1748 [00:30<00:22, 38.33it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.22.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  50%|█████     | 882/1748 [00:30<00:15, 57.64it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.23.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  51%|█████     | 891/1748 [00:31<00:26, 32.49it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.23.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  51%|█████▏    | 898/1748 [00:31<00:30, 27.65it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.23.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  52%|█████▏    | 903/1748 [00:31<00:29, 29.06it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.23.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  53%|█████▎    | 922/1748 [00:32<00:18, 44.02it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.23.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  54%|█████▎    | 938/1748 [00:32<00:21, 38.46it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.24.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  54%|█████▍    | 943/1748 [00:32<00:21, 37.02it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.24.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  54%|█████▍    | 952/1748 [00:33<00:33, 23.44it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.24.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  55%|█████▍    | 955/1748 [00:33<00:34, 23.21it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.24.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  56%|█████▌    | 976/1748 [00:33<00:17, 44.50it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.24.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  57%|█████▋    | 992/1748 [00:33<00:11, 67.16it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.25.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  57%|█████▋    | 1001/1748 [00:34<00:21, 34.46it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.25.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.25.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  58%|█████▊    | 1008/1748 [00:34<00:29, 25.33it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.25.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  59%|█████▉    | 1032/1748 [00:35<00:17, 40.19it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.25.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  60%|█████▉    | 1046/1748 [00:35<00:15, 44.07it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.26.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  60%|██████    | 1052/1748 [00:35<00:20, 33.53it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.26.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  60%|██████    | 1057/1748 [00:35<00:20, 33.24it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.26.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  61%|██████    | 1062/1748 [00:36<00:26, 25.60it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.26.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  62%|██████▏   | 1084/1748 [00:36<00:15, 42.84it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.26.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  63%|██████▎   | 1098/1748 [00:36<00:10, 59.41it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.27.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  63%|██████▎   | 1107/1748 [00:37<00:15, 40.92it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.27.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  64%|██████▎   | 1114/1748 [00:37<00:24, 25.91it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.27.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  64%|██████▍   | 1119/1748 [00:37<00:22, 27.43it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.27.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  65%|██████▌   | 1137/1748 [00:38<00:15, 40.37it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.27.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  66%|██████▌   | 1154/1748 [00:38<00:13, 44.03it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.28.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  66%|██████▋   | 1160/1748 [00:38<00:14, 41.25it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.28.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  67%|██████▋   | 1166/1748 [00:43<02:15,  4.31it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.28.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  67%|██████▋   | 1170/1748 [00:44<02:00,  4.81it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.28.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  68%|██████▊   | 1193/1748 [00:44<00:44, 12.34it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.28.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  69%|██████▉   | 1209/1748 [00:44<00:28, 18.62it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.29.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  70%|██████▉   | 1215/1748 [00:45<00:30, 17.40it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.29.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  70%|██████▉   | 1220/1748 [00:45<00:28, 18.81it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.29.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  70%|███████   | 1225/1748 [00:45<00:30, 17.38it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.29.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  71%|███████▏  | 1247/1748 [00:45<00:14, 34.14it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.29.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  72%|███████▏  | 1263/1748 [00:46<00:14, 33.92it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.3.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  73%|███████▎  | 1269/1748 [00:46<00:17, 26.73it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.3.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  73%|███████▎  | 1274/1748 [00:46<00:21, 22.27it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.3.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  73%|███████▎  | 1278/1748 [00:47<00:20, 22.88it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.3.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  75%|███████▍  | 1306/1748 [00:47<00:08, 51.83it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.3.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  75%|███████▌  | 1317/1748 [00:47<00:09, 47.64it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.30.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  76%|███████▌  | 1324/1748 [00:48<00:12, 34.49it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.30.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  76%|███████▌  | 1329/1748 [00:48<00:12, 33.41it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.30.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  76%|███████▋  | 1334/1748 [00:48<00:15, 25.94it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.30.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  78%|███████▊  | 1357/1748 [00:48<00:08, 45.84it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.30.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  78%|███████▊  | 1370/1748 [00:49<00:07, 47.91it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.31.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  79%|███████▊  | 1376/1748 [00:49<00:10, 34.12it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.31.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  79%|███████▉  | 1381/1748 [00:49<00:10, 33.66it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.31.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  79%|███████▉  | 1386/1748 [00:49<00:14, 25.61it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.31.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  81%|████████  | 1409/1748 [00:50<00:07, 45.08it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.31.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  82%|████████▏ | 1425/1748 [00:50<00:07, 42.92it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.4.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  82%|████████▏ | 1431/1748 [00:50<00:07, 40.61it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.4.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  82%|████████▏ | 1436/1748 [00:51<00:11, 28.24it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.4.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  82%|████████▏ | 1440/1748 [00:51<00:14, 21.60it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.4.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  84%|████████▎ | 1462/1748 [00:51<00:07, 40.18it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.4.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  85%|████████▍ | 1478/1748 [00:52<00:06, 44.90it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.5.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  85%|████████▍ | 1484/1748 [00:52<00:07, 33.41it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.5.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  85%|████████▌ | 1489/1748 [00:52<00:07, 32.97it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.5.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  85%|████████▌ | 1494/1748 [00:52<00:10, 24.89it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.5.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  87%|████████▋ | 1516/1748 [00:53<00:05, 42.89it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.5.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  88%|████████▊ | 1532/1748 [00:53<00:03, 62.12it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.6.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  88%|████████▊ | 1541/1748 [00:53<00:06, 33.69it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.6.mlp.gate_proj.weight due to size mismatch\n",
            "WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.6.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  89%|████████▊ | 1548/1748 [00:54<00:07, 25.41it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.6.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  90%|████████▉ | 1571/1748 [00:54<00:04, 41.09it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.6.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  91%|█████████ | 1586/1748 [00:54<00:03, 43.58it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.7.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  91%|█████████ | 1593/1748 [00:55<00:03, 42.34it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.7.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  91%|█████████▏| 1599/1748 [00:55<00:05, 26.27it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.7.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  92%|█████████▏| 1604/1748 [00:55<00:05, 27.49it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.7.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  93%|█████████▎| 1624/1748 [00:56<00:02, 41.57it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.7.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  94%|█████████▍| 1639/1748 [00:56<00:01, 59.87it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.8.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  94%|█████████▍| 1648/1748 [00:56<00:03, 32.74it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.8.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  95%|█████████▍| 1655/1748 [00:57<00:03, 27.95it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.8.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  95%|█████████▍| 1660/1748 [00:57<00:03, 29.06it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.8.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  96%|█████████▌| 1679/1748 [00:57<00:01, 42.78it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.8.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph:  97%|█████████▋| 1693/1748 [00:57<00:00, 59.93it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.9.mlp.down_proj.weight due to size mismatch\n",
            "Executing graph:  97%|█████████▋| 1702/1748 [00:58<00:01, 32.39it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.9.mlp.gate_proj.weight due to size mismatch\n",
            "Executing graph:  98%|█████████▊| 1709/1748 [00:58<00:01, 27.44it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.9.mlp.up_proj.weight due to size mismatch\n",
            "Executing graph:  98%|█████████▊| 1714/1748 [00:58<00:01, 28.72it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.9.self_attn.k_proj.weight due to size mismatch\n",
            "Executing graph:  99%|█████████▉| 1732/1748 [00:59<00:00, 41.67it/s]WARNING:root:skipping codellama/CodeLlama-7b-Instruct-hf:model.layers.9.self_attn.v_proj.weight due to size mismatch\n",
            "Executing graph: 100%|██████████| 1748/1748 [01:03<00:00, 27.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# actually do merge\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "from mergekit.config import MergeConfiguration\n",
        "from mergekit.merge import MergeOptions, run_merge\n",
        "\n",
        "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
        "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
        "\n",
        "run_merge(\n",
        "    merge_config,\n",
        "    out_path=OUTPUT_PATH,\n",
        "    options=MergeOptions(\n",
        "        lora_merge_cache=LORA_MERGE_CACHE,\n",
        "        cuda=torch.cuda.is_available(),\n",
        "        copy_tokenizer=COPY_TOKENIZER,\n",
        "        lazy_unpickle=LAZY_UNPICKLE,\n",
        "        low_cpu_memory=LOW_CPU_MEMORY,\n",
        "        allow_crimes=ALLOW_CRIMES,\n",
        "        write_model_card=True,\n",
        "    ),\n",
        ")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "* 7b * 3 model merge\n",
        "* download 5-10 minutes, merging 6 minutes\n",
        "* model size : around 14G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<code>\n",
        "(venv) iddx@idvpds:~/workspace/mergekit/ultra_llm_merged$ ls -alh\n",
        "total 14G\n",
        "drwxrwxr-x  2 iddx iddx 4.0K  5월 28 11:41 .\n",
        "drwxrwxr-x 12 iddx iddx 4.0K  5월 28 11:40 ..\n",
        "-rw-rw-r--  1 iddx iddx  641  5월 28 11:41 config.json\n",
        "-rw-rw-r--  1 iddx iddx  493  5월 28 11:41 mergekit_config.yml\n",
        "-rw-rw-r--  1 iddx iddx 4.6G  5월 28 11:40 model-00001-of-00003.safetensors\n",
        "-rw-rw-r--  1 iddx iddx 4.6G  5월 28 11:40 model-00002-of-00003.safetensors\n",
        "-rw-rw-r--  1 iddx iddx 4.4G  5월 28 11:41 model-00003-of-00003.safetensors\n",
        "-rw-rw-r--  1 iddx iddx  23K  5월 28 11:41 model.safetensors.index.json\n",
        "-rw-rw-r--  1 iddx iddx 1.4K  5월 28 11:41 README.md\n",
        "-rw-rw-r--  1 iddx iddx  414  5월 28 11:41 special_tokens_map.json\n",
        "-rw-rw-r--  1 iddx iddx  967  5월 28 11:41 tokenizer_config.json\n",
        "-rw-rw-r--  1 iddx iddx 1.8M  5월 28 11:41 tokenizer.json\n",
        "-rw-rw-r--  1 iddx iddx 482K  5월 28 11:41 tokenizer.model\n",
        "</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usage:  huggingface-cli upload [repo_id] [local_path] [path_in_repo]\n",
        "# %huggingface-cli upload ultra_llm_merged ./ultra_llm_merged ."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "a82552466edc3c939382dfc1503b75e8bf4e63d9975854a03fb17a3a3732fc62"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
